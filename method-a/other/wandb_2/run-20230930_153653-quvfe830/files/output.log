Generations:   0%|                                                                                                                                                                         | 0/20 [00:00<?, ?it/s]

Traceback (most recent call last):
  File "/Users/leo/Programming/PLR/Leo/gen_algo_opt_folder/genetic_algorithm.py", line 281, in <module>
    best_individual, best_fitness = ga.run()
                                    ^^^^^^^^
  File "/Users/leo/Programming/PLR/Leo/gen_algo_opt_folder/genetic_algorithm.py", line 208, in run
    fitness_val = self.fitness(individual)
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/leo/Programming/PLR/Leo/gen_algo_opt_folder/genetic_algorithm.py", line 76, in fitness
    optimizer.step()
  File "/Users/leo/Programming/PLR/Leo/env/lib/python3.11/site-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/leo/Programming/PLR/Leo/env/lib/python3.11/site-packages/torch/optim/optimizer.py", line 33, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/leo/Programming/PLR/Leo/env/lib/python3.11/site-packages/torch/optim/adam.py", line 141, in step
    adam(
  File "/Users/leo/Programming/PLR/Leo/env/lib/python3.11/site-packages/torch/optim/adam.py", line 281, in adam
    func(params,
  File "/Users/leo/Programming/PLR/Leo/env/lib/python3.11/site-packages/torch/optim/adam.py", line 344, in _single_tensor_adam
    exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
KeyboardInterrupt