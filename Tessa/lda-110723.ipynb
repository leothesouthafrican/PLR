{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDAmodel\n",
    "from sklearn.model_selection import KFold, ParameterGrid\n",
    "import seaborn as sns\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/cleaned_data_SYMPTOMS_9_13_23.csv\", index_col=0)\n",
    "sympdf = df.loc[:, df.columns.str.startswith('Symptom_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_components': [3, 4, 5, 6, 7],\n",
    "    'learning_method': ['batch'],\n",
    "    'random_state': [42],\n",
    "    'max_iter': [5, 10, 20],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_method': 'batch', 'max_iter': 5, 'n_components': 3, 'random_state': 42}\n",
      "{'learning_method': 'batch', 'max_iter': 5, 'n_components': 4, 'random_state': 42}\n",
      "{'learning_method': 'batch', 'max_iter': 5, 'n_components': 5, 'random_state': 42}\n",
      "{'learning_method': 'batch', 'max_iter': 5, 'n_components': 6, 'random_state': 42}\n",
      "{'learning_method': 'batch', 'max_iter': 5, 'n_components': 7, 'random_state': 42}\n",
      "{'learning_method': 'batch', 'max_iter': 10, 'n_components': 3, 'random_state': 42}\n",
      "{'learning_method': 'batch', 'max_iter': 10, 'n_components': 4, 'random_state': 42}\n",
      "{'learning_method': 'batch', 'max_iter': 10, 'n_components': 5, 'random_state': 42}\n",
      "{'learning_method': 'batch', 'max_iter': 10, 'n_components': 6, 'random_state': 42}\n",
      "{'learning_method': 'batch', 'max_iter': 10, 'n_components': 7, 'random_state': 42}\n",
      "{'learning_method': 'batch', 'max_iter': 20, 'n_components': 3, 'random_state': 42}\n",
      "{'learning_method': 'batch', 'max_iter': 20, 'n_components': 4, 'random_state': 42}\n",
      "{'learning_method': 'batch', 'max_iter': 20, 'n_components': 5, 'random_state': 42}\n",
      "{'learning_method': 'batch', 'max_iter': 20, 'n_components': 6, 'random_state': 42}\n",
      "{'learning_method': 'batch', 'max_iter': 20, 'n_components': 7, 'random_state': 42}\n",
      "Best hyperparameters: {'learning_method': 'batch', 'max_iter': 20, 'n_components': 3, 'random_state': 42}\n",
      "Best average perplexity: 133.0275850554372\n"
     ]
    }
   ],
   "source": [
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=0)\n",
    "results = {}  # To store results\n",
    "\n",
    "for hyperparameters in ParameterGrid(param_grid):\n",
    "    print(hyperparameters)\n",
    "    total_perplexity = 0\n",
    "    lda = LDAmodel(**hyperparameters)\n",
    "    \n",
    "    perplexities = []  # To store perplexity values for each fold\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(sympdf), 1):\n",
    "        train_data = sympdf.iloc[train_index]\n",
    "        val_data = sympdf.iloc[val_index]\n",
    "        \n",
    "        lda.fit(train_data)\n",
    "        perplexity = lda.perplexity(val_data)\n",
    "        perplexities.append(perplexity)\n",
    "\n",
    "    average_perplexity = np.mean(perplexities)\n",
    "\n",
    "    # Store the results\n",
    "    results[str(hyperparameters)] = {\n",
    "        \"average_perplexity\": average_perplexity,\n",
    "        \"perplexities\": perplexities\n",
    "    }\n",
    "\n",
    "# Find the best hyperparameters based on average perplexity\n",
    "best_hyperparameters = min(results, key=lambda x: results[x][\"average_perplexity\"])\n",
    "best_average_perplexity = results[best_hyperparameters][\"average_perplexity\"]\n",
    "\n",
    "print(\"Best hyperparameters:\", best_hyperparameters)\n",
    "print(\"Best average perplexity:\", best_average_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a pkl file of results\n",
    "with open('output/lda_results-4.pkl', 'wb') as f:\n",
    "    pkl.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'n_components': 10, 'learning_decay': 0.9}\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# performed well with a default learning decay, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the average_perplexities into a dataframe\n",
    "df = pd.DataFrame.from_dict(results, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='average_perplexity', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average_perplexity</th>\n",
       "      <th>perplexities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>{'learning_method': 'batch', 'max_iter': 20, 'n_components': 3, 'random_state': 42}</th>\n",
       "      <td>133.027585</td>\n",
       "      <td>[132.6451185286299, 132.86677982910615, 133.07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'learning_method': 'batch', 'max_iter': 10, 'n_components': 3, 'random_state': 42}</th>\n",
       "      <td>133.559102</td>\n",
       "      <td>[133.20260966265667, 133.45574274326054, 133.7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'learning_method': 'batch', 'max_iter': 5, 'n_components': 3, 'random_state': 42}</th>\n",
       "      <td>135.459295</td>\n",
       "      <td>[135.107787124454, 135.47688330452698, 135.886...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'learning_method': 'batch', 'max_iter': 20, 'n_components': 4, 'random_state': 42}</th>\n",
       "      <td>136.031815</td>\n",
       "      <td>[135.82686061987795, 135.72110708919075, 136.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'learning_method': 'batch', 'max_iter': 10, 'n_components': 4, 'random_state': 42}</th>\n",
       "      <td>136.417750</td>\n",
       "      <td>[136.1859146159476, 136.12856546788757, 136.71...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'learning_method': 'batch', 'max_iter': 5, 'n_components': 4, 'random_state': 42}</th>\n",
       "      <td>137.414989</td>\n",
       "      <td>[137.1714754909619, 137.19849219990073, 137.70...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'learning_method': 'batch', 'max_iter': 20, 'n_components': 5, 'random_state': 42}</th>\n",
       "      <td>138.621205</td>\n",
       "      <td>[138.59161110991235, 138.4524391366244, 138.74...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'learning_method': 'batch', 'max_iter': 10, 'n_components': 5, 'random_state': 42}</th>\n",
       "      <td>139.206151</td>\n",
       "      <td>[139.2100476223345, 138.95417047699462, 139.33...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'learning_method': 'batch', 'max_iter': 5, 'n_components': 5, 'random_state': 42}</th>\n",
       "      <td>140.419156</td>\n",
       "      <td>[140.3827892418617, 140.11843621605814, 140.47...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'learning_method': 'batch', 'max_iter': 20, 'n_components': 6, 'random_state': 42}</th>\n",
       "      <td>141.379701</td>\n",
       "      <td>[141.29878215806875, 141.30779888897487, 141.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'learning_method': 'batch', 'max_iter': 10, 'n_components': 6, 'random_state': 42}</th>\n",
       "      <td>141.779185</td>\n",
       "      <td>[141.6951342785997, 141.6299838361356, 141.971...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'learning_method': 'batch', 'max_iter': 5, 'n_components': 6, 'random_state': 42}</th>\n",
       "      <td>142.672266</td>\n",
       "      <td>[142.5480523659768, 142.605385168076, 142.9203...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'learning_method': 'batch', 'max_iter': 20, 'n_components': 7, 'random_state': 42}</th>\n",
       "      <td>144.035271</td>\n",
       "      <td>[144.02832691216938, 143.9572928906513, 144.27...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'learning_method': 'batch', 'max_iter': 10, 'n_components': 7, 'random_state': 42}</th>\n",
       "      <td>144.373024</td>\n",
       "      <td>[144.26393296874915, 144.2557128176617, 144.51...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'learning_method': 'batch', 'max_iter': 5, 'n_components': 7, 'random_state': 42}</th>\n",
       "      <td>145.249676</td>\n",
       "      <td>[145.12645190786273, 145.11532922198091, 145.4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    average_perplexity  \\\n",
       "{'learning_method': 'batch', 'max_iter': 20, 'n...          133.027585   \n",
       "{'learning_method': 'batch', 'max_iter': 10, 'n...          133.559102   \n",
       "{'learning_method': 'batch', 'max_iter': 5, 'n_...          135.459295   \n",
       "{'learning_method': 'batch', 'max_iter': 20, 'n...          136.031815   \n",
       "{'learning_method': 'batch', 'max_iter': 10, 'n...          136.417750   \n",
       "{'learning_method': 'batch', 'max_iter': 5, 'n_...          137.414989   \n",
       "{'learning_method': 'batch', 'max_iter': 20, 'n...          138.621205   \n",
       "{'learning_method': 'batch', 'max_iter': 10, 'n...          139.206151   \n",
       "{'learning_method': 'batch', 'max_iter': 5, 'n_...          140.419156   \n",
       "{'learning_method': 'batch', 'max_iter': 20, 'n...          141.379701   \n",
       "{'learning_method': 'batch', 'max_iter': 10, 'n...          141.779185   \n",
       "{'learning_method': 'batch', 'max_iter': 5, 'n_...          142.672266   \n",
       "{'learning_method': 'batch', 'max_iter': 20, 'n...          144.035271   \n",
       "{'learning_method': 'batch', 'max_iter': 10, 'n...          144.373024   \n",
       "{'learning_method': 'batch', 'max_iter': 5, 'n_...          145.249676   \n",
       "\n",
       "                                                                                         perplexities  \n",
       "{'learning_method': 'batch', 'max_iter': 20, 'n...  [132.6451185286299, 132.86677982910615, 133.07...  \n",
       "{'learning_method': 'batch', 'max_iter': 10, 'n...  [133.20260966265667, 133.45574274326054, 133.7...  \n",
       "{'learning_method': 'batch', 'max_iter': 5, 'n_...  [135.107787124454, 135.47688330452698, 135.886...  \n",
       "{'learning_method': 'batch', 'max_iter': 20, 'n...  [135.82686061987795, 135.72110708919075, 136.2...  \n",
       "{'learning_method': 'batch', 'max_iter': 10, 'n...  [136.1859146159476, 136.12856546788757, 136.71...  \n",
       "{'learning_method': 'batch', 'max_iter': 5, 'n_...  [137.1714754909619, 137.19849219990073, 137.70...  \n",
       "{'learning_method': 'batch', 'max_iter': 20, 'n...  [138.59161110991235, 138.4524391366244, 138.74...  \n",
       "{'learning_method': 'batch', 'max_iter': 10, 'n...  [139.2100476223345, 138.95417047699462, 139.33...  \n",
       "{'learning_method': 'batch', 'max_iter': 5, 'n_...  [140.3827892418617, 140.11843621605814, 140.47...  \n",
       "{'learning_method': 'batch', 'max_iter': 20, 'n...  [141.29878215806875, 141.30779888897487, 141.5...  \n",
       "{'learning_method': 'batch', 'max_iter': 10, 'n...  [141.6951342785997, 141.6299838361356, 141.971...  \n",
       "{'learning_method': 'batch', 'max_iter': 5, 'n_...  [142.5480523659768, 142.605385168076, 142.9203...  \n",
       "{'learning_method': 'batch', 'max_iter': 20, 'n...  [144.02832691216938, 143.9572928906513, 144.27...  \n",
       "{'learning_method': 'batch', 'max_iter': 10, 'n...  [144.26393296874915, 144.2557128176617, 144.51...  \n",
       "{'learning_method': 'batch', 'max_iter': 5, 'n_...  [145.12645190786273, 145.11532922198091, 145.4...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df\n",
    "# takeaway: n_components of 4 is across the board the best performing \n",
    "# What does it mean that I'm getting the same values? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# typically, you'd expect more topics means more model capacity \n",
    "# so why is my perplexity dropping? \n",
    "# are some patients having really poor fit? \n",
    "# or maybe need to do symptom pruning analogous to stopwords\n",
    "\n",
    "# symptom absence tells you something about a patient\n",
    "\n",
    "# think about different prior? \n",
    "# \n",
    "\n",
    "#doc_topic_priorfloat, default=None\n",
    "#Prior of document topic distribution theta. If the value is None, defaults to 1 / n_components. In [1], this is called alpha.\n",
    "# dirichlet prior -- flat by default\n",
    "# if we have reason to believe docs are spiky, you'd want this to be a lower value \n",
    "# try running with (0.5,0.5,0.5 , ... ) and look if this is input as a number or a vector\n",
    "\n",
    "#topic_word_priorfloat, default=None\n",
    "#Prior of topic word distribution beta. If the value is None, defaults to 1 / n_components. In [1], this is called eta.\n",
    "# leave this -- expectation is mixed\n",
    "# should probably be 1/n_words (/library size) but look at the implementation bc this seems weird\n",
    "\n",
    "\n",
    "# possible issues: model is a poor fit for the data (which we know) so perplexity might not be the best readout\n",
    "# priors might be off\n",
    "# need to look at the topics themselves and the topic dists for patients and see if they make sense"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nimfa",
   "language": "python",
   "name": "nimfa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
