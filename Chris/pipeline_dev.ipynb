{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f69f860-0864-4572-8ec0-c879972a64d3",
   "metadata": {},
   "source": [
    "#### In this notebook we develop a pipeline for hyperparameter tuning for UMAP + HDBSCAN.\n",
    "\n",
    "We need to tune the following params:\n",
    "\n",
    "UMAP:\n",
    "- n_neighbors: [2, 0.25*len(df)]\n",
    "- min_dist: [0, 0.99]\n",
    "- n_components: [2, n_features]\n",
    "- metric: [9 metrics for binary data]\n",
    "\n",
    "HDBSCAN:\n",
    "- min_cluster_size:\n",
    "- min_samples: \n",
    "Note: If you wish to explore different min_cluster_size settings with a fixed min_samples value, especially for larger dataset sizes, you can cache the hard computation, and recompute only the relatively cheap flat cluster extraction using the memory parameter, which makes use of joblib\n",
    "- cluster_selection_epsilon: ?\n",
    "[- alpha]X\n",
    "[- leaf clustering, not EOM]\n",
    "\n",
    "\n",
    "##### Here we use the DBCV score, but could try others?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55db4eab-d151-4738-a7f3-f3ad0674dacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c8599b5-5061-43d1-966e-b01b39338a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import load_symptom_data\n",
    "import hdbscan\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import wandb\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b3aedf4-1c61-4930-b872-2e1e9ade306f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_symptom_data('../data/cleaned_data_SYMPTOMS_9_13_23.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf1b003-b34f-433a-a901-2075a16c551c",
   "metadata": {},
   "source": [
    "##### Trying different approach to rescue grid search!\n",
    "\n",
    "- To get GridSearchCV to fit and return the score for the full dataset, we need to use a predefined split with one copy of the data fro training and another copy for validation.\n",
    "- We need to create our own scoring function with the correct signature (i.e. no need for y_true), as below.\n",
    "- Need to make sure refit=False\n",
    "- Need to make sure that random state is the same for each split ???\n",
    "\n",
    "\n",
    "### Questions: should DVBC score use local value of 'metric' - problematic for comparing across different runs...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fff5d24-7ef8-4767-983d-911aad5f61e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b576aaeb-c715-4bbc-9134-a545354738aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrusty-chris\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rustybilges/Documents/Work/PLRC/PLR/Chris/wandb/run-20231001_100533-9uju4fgn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rusty-chris/test_clulster/runs/9uju4fgn' target=\"_blank\">run_test_hdb_dbcv</a></strong> to <a href='https://wandb.ai/rusty-chris/test_clulster' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rusty-chris/test_clulster' target=\"_blank\">https://wandb.ai/rusty-chris/test_clulster</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rusty-chris/test_clulster/runs/9uju4fgn' target=\"_blank\">https://wandb.ai/rusty-chris/test_clulster/runs/9uju4fgn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "        name='run_test_hdb_dbcv',\n",
    "        project='test_clulster',\n",
    "        config={}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e6e9b1b-4be2-4057-a26a-84039a75f3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = pd.concat([df, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe429f92-6f36-4d55-afa4-bb5efa1956e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = PredefinedSplit([0 if i < len(df) else 1 for i in range(len(ddf.index))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc746d52-a04f-41ec-b2f1-2469e1fa3efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = np.array([0 if i < len(df) else 1 for i in ddf.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "231b984a-9123-404d-9e43-605d58a36ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0d6ded5-e6e7-4fef-a729-3feb6e5d03da",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdb = hdbscan.HDBSCAN(gen_min_span_tree=True, core_dist_n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49c4344f-fc06-4308-8f84-8814cc9e1d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_init='auto', random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1407638-72cf-41cc-ab2f-1020076744a4",
   "metadata": {},
   "source": [
    "##### Notes: \n",
    "- getting overflow in this version of DBCV when distances are small. Is there another implementation we can use?\n",
    "- this code may not work for n_jobs!=1 because of the way we obtain the iterations number from the length of the otpimisation result.\n",
    "- have to downgrade to skopt==0.8.1 and sklearn=0.24.2 for correct behaviour with best_scores_ (and other features)?\n",
    "- these scores use model.steps[1][1].labels_ instead of model.steps.labels_ because they are accessing the clustering model which is the second step in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a40c2f0-8de3-476e-9b04-1cb2d7863368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbcv(data, labels, metric='euclidean'):\n",
    "    \n",
    "    if metric == None:\n",
    "        metric = model.steps[1][1].get_params()['metric']\n",
    "        \n",
    "    return hdbscan.validity.validity_index(\n",
    "            data, labels,\n",
    "            metric=metric\n",
    "        )\n",
    "\n",
    "def dbcv_manhattan(data, labels):\n",
    "    return dbcv(data, labels, metric='manhattan')\n",
    "\n",
    "def silhouette(data, labels):\n",
    "    num_labels = len(set(labels))\n",
    "    if num_labels == 1:\n",
    "        print(\"Warning: Valid number of clusters must be 2 or more.\")\n",
    "        return 0\n",
    "    else:\n",
    "        return silhouette_score(data, labels)\n",
    "\n",
    "def calinski_harabasz(data, labels):\n",
    "    num_labels = len(set(labels))\n",
    "    if num_labels == 1:\n",
    "        print(\"Warning: Valid number of clusters must be 2 or more.\")\n",
    "        return 0\n",
    "    else:\n",
    "        return calinski_harabasz_score(data, labels)\n",
    "\n",
    "def davies_bouldin(data, labels):\n",
    "    \"\"\"\n",
    "    Note: 0 is best. If using for CV need to use complement.\n",
    "    \"\"\"\n",
    "    num_labels = len(set(labels))\n",
    "    if num_labels == 1:\n",
    "        print(\"Warning: Valid number of clusters must be 2 or more.\")\n",
    "        return 1\n",
    "    else:\n",
    "        return davies_bouldin_score(data, labels)\n",
    "\n",
    "def cv_score(model, X, score='dbcv'):\n",
    "    \"\"\"\n",
    "    If score == 'all' we return a dictionary of all scores, which\n",
    "    can be logged to wandb on each iteration. \n",
    "\n",
    "    Otherwise this is intended for use as a scorer in <X>SearchCV methods.\n",
    "    In that case metric should be fixed to allow comparison across different runs.\n",
    "    \"\"\"\n",
    "    score_dict = {\n",
    "        'silhouette': silhouette,\n",
    "        'dbcv': dbcv,\n",
    "        'calinski_harabasz': calinski_harabasz,\n",
    "        'davies_bouldin': davies_bouldin\n",
    "    }\n",
    "    \n",
    "    model.fit(X)\n",
    "    labels = model.steps[1][1].labels_\n",
    "    data = model.steps[0][1].transform(X)\n",
    "\n",
    "    if score == 'all':\n",
    "        return {\n",
    "            score_name: score_func(data, labels) \n",
    "            for score_name, score_func in score_dict.items()\n",
    "        }\n",
    "    else:\n",
    "        return score_dict[score](data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc0d0abf-e3e2-432f-9343-4f6ab6e0a685",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params = {\n",
    "    'pca__n_components': Integer(5, 150),\n",
    "    'hdbscan__cluster_selection_epsilon' : Real(0.0, 100.0),\n",
    "    'hdbscan__cluster_selection_method' : Categorical(['eom', 'leaf']),\n",
    "    'hdbscan__metric' : Categorical(['euclidean', 'manhattan']),\n",
    "    'hdbscan__min_cluster_size':Integer(10, 2000),  \n",
    "    'hdbscan__min_samples': Integer(1,1000),\n",
    "    \n",
    "}\n",
    "\n",
    "# hyper_params = {\n",
    "#     'pca__n_components': [5, 15],#, 30, 45, 60],\n",
    "#     'kmeans__n_clusters': Integer(2, 20)\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07247f54-a057-440c-80e0-259f5fb1e99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[('pca', pca), ('hdbscan', hdb)])\n",
    "# pipe = Pipeline(steps=[('pca', pca), ('kmeans', kmeans)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea5f6e32-6307-47ee-85c0-123916e6b9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tunning = BayesSearchCV(\n",
    "   estimator=pipe,\n",
    "   search_spaces=hyper_params,\n",
    "   scoring=cv_score,\n",
    "   cv=split,\n",
    "   n_jobs=-1,\n",
    "   refit=False,\n",
    "   return_train_score=True,\n",
    "   n_iter=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a4cfca9-804e-4acc-bc60-d5ccbc77e486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hdbscan__cluster_selection_epsilon',\n",
       " 'hdbscan__cluster_selection_method',\n",
       " 'hdbscan__metric',\n",
       " 'hdbscan__min_cluster_size',\n",
       " 'hdbscan__min_samples',\n",
       " 'pca__n_components']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(hyper_params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "528a3b1b-8a52-41cc-bd6d-2db6834b737e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cv': PredefinedSplit(test_fold=array([0, 0, ..., 1, 1])),\n",
       " 'error_score': 'raise',\n",
       " 'estimator__memory': None,\n",
       " 'estimator__steps': [('pca', PCA(random_state=42)),\n",
       "  ('hdbscan', HDBSCAN(gen_min_span_tree=True))],\n",
       " 'estimator__verbose': False,\n",
       " 'estimator__pca': PCA(random_state=42),\n",
       " 'estimator__hdbscan': HDBSCAN(gen_min_span_tree=True),\n",
       " 'estimator__pca__copy': True,\n",
       " 'estimator__pca__iterated_power': 'auto',\n",
       " 'estimator__pca__n_components': None,\n",
       " 'estimator__pca__n_oversamples': 10,\n",
       " 'estimator__pca__power_iteration_normalizer': 'auto',\n",
       " 'estimator__pca__random_state': 42,\n",
       " 'estimator__pca__svd_solver': 'auto',\n",
       " 'estimator__pca__tol': 0.0,\n",
       " 'estimator__pca__whiten': False,\n",
       " 'estimator__hdbscan__algorithm': 'best',\n",
       " 'estimator__hdbscan__allow_single_cluster': False,\n",
       " 'estimator__hdbscan__alpha': 1.0,\n",
       " 'estimator__hdbscan__approx_min_span_tree': True,\n",
       " 'estimator__hdbscan__cluster_selection_epsilon': 0.0,\n",
       " 'estimator__hdbscan__cluster_selection_method': 'eom',\n",
       " 'estimator__hdbscan__core_dist_n_jobs': 4,\n",
       " 'estimator__hdbscan__gen_min_span_tree': True,\n",
       " 'estimator__hdbscan__leaf_size': 40,\n",
       " 'estimator__hdbscan__match_reference_implementation': False,\n",
       " 'estimator__hdbscan__max_cluster_size': 0,\n",
       " 'estimator__hdbscan__memory': Memory(location=None),\n",
       " 'estimator__hdbscan__metric': 'euclidean',\n",
       " 'estimator__hdbscan__min_cluster_size': 5,\n",
       " 'estimator__hdbscan__min_samples': None,\n",
       " 'estimator__hdbscan__p': None,\n",
       " 'estimator__hdbscan__prediction_data': False,\n",
       " 'estimator': Pipeline(steps=[('pca', PCA(random_state=42)),\n",
       "                 ('hdbscan', HDBSCAN(gen_min_span_tree=True))]),\n",
       " 'fit_params': None,\n",
       " 'iid': 'deprecated',\n",
       " 'n_iter': 1,\n",
       " 'n_jobs': -1,\n",
       " 'n_points': 1,\n",
       " 'optimizer_kwargs': None,\n",
       " 'pre_dispatch': '2*n_jobs',\n",
       " 'random_state': None,\n",
       " 'refit': False,\n",
       " 'return_train_score': True,\n",
       " 'scoring': <function __main__.cv_score(model, X, score='dbcv')>,\n",
       " 'search_spaces': {'pca__n_components': Integer(low=5, high=150, prior='uniform', transform='normalize'),\n",
       "  'hdbscan__cluster_selection_epsilon': Real(low=0.0, high=100.0, prior='uniform', transform='normalize'),\n",
       "  'hdbscan__cluster_selection_method': Categorical(categories=('eom', 'leaf'), prior=None),\n",
       "  'hdbscan__metric': Categorical(categories=('euclidean', 'manhattan'), prior=None),\n",
       "  'hdbscan__min_cluster_size': Integer(low=10, high=2000, prior='uniform', transform='normalize'),\n",
       "  'hdbscan__min_samples': Integer(low=1, high=1000, prior='uniform', transform='normalize')},\n",
       " 'verbose': 0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tunning.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8a3ab910-125f-4e69-a9b5-d48d062c78fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tunning.total_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60dedc78-362c-4012-8fc8-f3e2d22947d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wandb_callback(result):\n",
    "    iter = len(result['x_iters'])\n",
    "    print('Iteration %d' %iter)\n",
    "    print(result)\n",
    "    assert False\n",
    "    current_params = dict(zip(\n",
    "        hyper_params.keys(), \n",
    "        result['x_iters'][-1]\n",
    "    ))\n",
    "    pipe.set_params(**current_params)\n",
    "    all_scores = cv_score(pipe, df, score='all')\n",
    "    \n",
    "    log_dict = {\n",
    "        'best_score': -result['fun'],\n",
    "        'best_params': result['x'],\n",
    "        'current_params': current_params\n",
    "    }\n",
    "    for key in all_scores.keys():\n",
    "        log_dict[key] = all_scores[key]\n",
    "\n",
    "    run.log(log_dict)\n",
    "    print(log_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fce8dae4-9b84-41e0-9088-89438a143f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "          fun: -0.0\n",
      "            x: [6.450946819875848, 'leaf', 'euclidean', 1706, 981, 126]\n",
      "    func_vals: [-0.000e+00]\n",
      "      x_iters: [[6.450946819875848, 'leaf', 'euclidean', 1706, 981, 126]]\n",
      "       models: []\n",
      "        space: Space([Real(low=0.0, high=100.0, prior='uniform', transform='normalize'),\n",
      "                      Categorical(categories=('eom', 'leaf'), prior=None),\n",
      "                      Categorical(categories=('euclidean', 'manhattan'), prior=None),\n",
      "                      Integer(low=10, high=2000, prior='uniform', transform='normalize'),\n",
      "                      Integer(low=1, high=1000, prior='uniform', transform='normalize'),\n",
      "                      Integer(low=5, high=150, prior='uniform', transform='normalize')])\n",
      " random_state: RandomState(MT19937)\n",
      "        specs:     args:              dimensions: [Real(low=0.0, high=100.0, prior='uniform', transform='normalize'), Categorical(categories=('eom', 'leaf'), prior=None), Categorical(categories=('euclidean', 'manhattan'), prior=None), Integer(low=10, high=2000, prior='uniform', transform='normalize'), Integer(low=1, high=1000, prior='uniform', transform='normalize'), Integer(low=5, high=150, prior='uniform', transform='normalize')]\n",
      "                                  base_estimator: gp\n",
      "                                 n_random_starts: None\n",
      "                                n_initial_points: 10\n",
      "                         initial_point_generator: random\n",
      "                                          n_jobs: 1\n",
      "                                        acq_func: gp_hedge\n",
      "                                   acq_optimizer: auto\n",
      "                                    random_state: RandomState(MT19937)\n",
      "                                model_queue_size: None\n",
      "                                 acq_func_kwargs: None\n",
      "                            acq_optimizer_kwargs: None\n",
      "               function: Optimizer\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtunning\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mddf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwandb_callback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(elapsed_time)\n",
      "File \u001b[0;32m~/Documents/Work/PLRC/PLR/Chris/venv/lib/python3.8/site-packages/skopt/searchcv.py:466\u001b[0m, in \u001b[0;36mBayesSearchCV.fit\u001b[0;34m(self, X, y, groups, callback, **fit_params)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_kwargs_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_kwargs)\n\u001b[0;32m--> 466\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;66;03m# BaseSearchCV never ranked train scores,\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;66;03m# but apparently we used to ship this (back-compat)\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_train_score:\n",
      "File \u001b[0;32m~/Documents/Work/PLRC/PLR/Chris/venv/lib/python3.8/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Work/PLRC/PLR/Chris/venv/lib/python3.8/site-packages/sklearn/model_selection/_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    894\u001b[0m     )\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 898\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/Work/PLRC/PLR/Chris/venv/lib/python3.8/site-packages/skopt/searchcv.py:518\u001b[0m, in \u001b[0;36mBayesSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m    512\u001b[0m     optim_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step(\n\u001b[1;32m    513\u001b[0m         search_space, optimizer,\n\u001b[1;32m    514\u001b[0m         evaluate_candidates, n_points\u001b[38;5;241m=\u001b[39mn_points_adjusted\n\u001b[1;32m    515\u001b[0m     )\n\u001b[1;32m    516\u001b[0m     n_iter \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m n_points\n\u001b[0;32m--> 518\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43meval_callbacks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim_result\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    519\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optim_results\u001b[38;5;241m.\u001b[39mappend(optim_result)\n",
      "File \u001b[0;32m~/Documents/Work/PLRC/PLR/Chris/venv/lib/python3.8/site-packages/skopt/utils.py:98\u001b[0m, in \u001b[0;36meval_callbacks\u001b[0;34m(callbacks, result)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callbacks:\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m callbacks:\n\u001b[0;32m---> 98\u001b[0m         decision \u001b[38;5;241m=\u001b[39m \u001b[43mc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m decision \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m             stop \u001b[38;5;241m=\u001b[39m stop \u001b[38;5;129;01mor\u001b[39;00m decision\n",
      "Cell \u001b[0;32mIn[24], line 5\u001b[0m, in \u001b[0;36mwandb_callback\u001b[0;34m(result)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIteration \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m\u001b[38;5;28miter\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      6\u001b[0m current_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m      7\u001b[0m     hyper_params\u001b[38;5;241m.\u001b[39mkeys(), \n\u001b[1;32m      8\u001b[0m     result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_iters\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      9\u001b[0m ))\n\u001b[1;32m     10\u001b[0m pipe\u001b[38;5;241m.\u001b[39mset_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcurrent_params)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "tunning.fit(ddf.to_numpy(), callback=wandb_callback)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "449cca87-8a96-4c72-94e4-801c3efa47b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_results_sanity_check(pipe, df, cv_results):\n",
    "\n",
    "    bs = tunning.best_score_\n",
    "    bp = tunning.best_params_\n",
    "    \n",
    "    pipe.set_params(**bp)\n",
    "\n",
    "    try:\n",
    "        assert bs == cv_score(pipe, df.to_numpy())\n",
    "    except:\n",
    "        print(bs, cv_score(pipe, df.to_numpy()))\n",
    "    bid = np.where(tunning.cv_results_['mean_test_score'] == bs)[0][0]\n",
    "\n",
    "    assert bp == tunning.cv_results_['params'][bid]\n",
    "    assert bs == tunning.cv_results_['split0_test_score'][bid]\n",
    "    assert bs == tunning.cv_results_['split1_test_score'][bid]\n",
    "    assert bs == tunning.cv_results_['split0_train_score'][bid]\n",
    "    assert bs == tunning.cv_results_['split1_train_score'][bid]\n",
    "\n",
    "    for i, s in enumerate(tunning.cv_results_['split0_test_score']):\n",
    "        assert (\n",
    "            s == tunning.cv_results_['split1_test_score'][i]\n",
    "        )\n",
    "\n",
    "    print(\"These search results passed all sanity checks. They are deterministic and consistent. :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4ea321f8-90ed-4df0-87b8-a263f6848928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These search results passed all sanity checks. They are deterministic and consistent. :)\n"
     ]
    }
   ],
   "source": [
    "cv_results_sanity_check(pipe, df, tunning.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f738283f-f35e-492d-9c66-e450f437b6cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_score(pipe, df.to_numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
